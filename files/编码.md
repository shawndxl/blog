JS 编码
-------

### 计算机本质是一个基于数字的编码的玩具

计算机最终存储的是海量的二进制的 0 和 1。

> IC的所有引脚，只有直流电压0V或5V两个状态。也就是说，IC的一个引脚，只能表示两个状态。IC的这个特性，决定了计算机的信息数据只能用二进制数来处理。

计算机处理信息的最小单位 —— 位(bit / binary digit)，即二进制中的一位。

二进制数的位数一般是8位、16位、32位……也就是8的倍数，这是因为计算机所处理的信息的基本单位是8位二进制数。

8位二进制数被称为一个字节(byte)。

位是最小单位，字节是基本单位。内存和磁盘都使用字节单位来存储和读写数据，使用位单位则无法读写数据。

> 32位微处理器，具有32个引脚以用于信息的输入和输出。也就是说一次可以处理32位（32位= 4字节）的二进制数信息。

程序中，即使是用十进制数和文字等记述信息，在编译后也会转换成二进制数的值。


### 进制

binary / 二进制：ES6 中以0b开始的数据表示2进制

octonary / 八进制：ES6 中以0o开始的数据表示8进制

decimal / 十进制：JS 中默认的数字为10进制

hex / 十六进制：JS 中以0x开始的数据表示16进制

> 0b 0o 0x 的写法中第一个为数字0，第二个为字母，字母不区分大小写

* parseInt(string[, radix])

> radix 参数范围2-36

```js
parseInt('111', 2); // 把字符串或者数字1111按照 2进制来解析，并且返回其10进制的值
// 7
console.log(0b111);
// 7
parseInt('111', 8); // 把字符串或者数字1111按照 8进制来解析，并且返回其10进制的值
// 73
console.log(0o111);
// 73
parseInt('111', 10); // 把字符串或者数字1111按照 10进制来解析，并且返回其10进制的值
// 111
parseInt('111', 16); // 把字符串或者数字1111按照 16进制来解析，并且返回其10进制的值
// 273
console.log(0x111);
// 273
```

<details>
<summary>其他</summary>

ES5的非严格模式中 对于以0开头，且其中没有大于7的数字，则会按照8进制计算，开启严格模式则会报错，ES6中必须要以0o开始

```js
// 非严格模式
07
// 7
08
// 8
09
// 9
010
// 8
081
// 81
071
// 57
``` 

```js
'use strict';
console.log(011);
// Uncaught SyntaxError: Octal literals are not allowed in strict mode.
```
</details>

* numObj.toString([radix])

> radix 指定要用于数字到字符串的转换的基数(从2到36)。如果未指定 radix 参数，则默认值为 10。

```js
(16).toString();
// "16"
(16).toString(2);
// "10000"
(16).toString(10);
// "16"
(16).toString(16);
// "10"
(16).toString(36);
// "g"
(0xff).toString();
// "255"
(0xff).toString(16);
// "ff"
```

所以如果我们把字符串 '111' 先按照16进制来解析，再把解析后返回的十进制数 273 转化为 16进制数来显示，则依然得到 '111'

```js
parseInt('111', 16).toString(16)
// "111"
```


### 编码

以上说了半天数字，为什么？因为，电脑只处理数字。那么字母怎么办？我们现在这些形形色色的内容怎么办？

人类有办法。既然我们可以用二进制0、1来表示2、3、4、5等数字，我们也可以用二级制来表示像a、b、c、d这样的52个字母（包括大写）、以及一些常用的符号（例如*、#、@等），但具体用哪些二进制数字表示哪个符号，当然每个人都可以约定自己的一套，这就叫编码。


ASCII [@see wikipedia](https://en.wikipedia.org/wiki/ASCII)

> 美国有关的标准化组织就出台了ASCII编码，统一规定了上述常用符号用哪些二进制数来表示，他们把所有的空 
格、标点符号、数字、大小写字母分别用连续的字节状态表示，一直编到了第127号。

> ASCII是(American Standard Code for Information Interchange，美国信息交换标准代码)缩写，而不是ASCⅡ(罗马数字2)，有很多人在这个地方产生误解。（包括我）

Extended ASCII Codes / 扩展字符集 [@see asciitable](http://www.asciitable.com/)

> 很多国家用的不是英文，他们的字母里有许多是ASCII里没有的，为了可以在计算机保存他们的文字，他们决定采用 127号之后的空位来表示这些新的字母、符号，还加入了很多画表格时需要用下到的横线、竖线、交叉等形状，一直把序号编到了最后一个状态255。从128到255这一页的字符集被称 扩展字符集。

GBK / GB2312

> 计算机是在美欧先发展的，等中国人们得到计算机时，已经没有可以利用的字节状态来表示汉字，况且有6000多个常用汉字需要保存呢。但是这难不倒智慧的中国人民，我们不客气地把那些127号之后的奇异符号们直接取消掉, 规定：一个小于127的字符的意义与原来相同，但两个大于127的字符连在一起时，就表示一个汉字，前面的一个字节（他称之为高字节）从0xA1用到 
0xF7，后面一个字节（低字节）从0xA1到0xFE，这样我们就可以组合出大约7000多个简体汉字了.于是就把这种汉字方案叫做 “GB2312”

> 结果扩展之后的编码方案被称为 GBK 标准，GBK包括了GB2312 的所有内容，同时又增加了近20000个新的汉字（包括繁体字）和符号。


Unicode

中国可以搞针对汉字的GBK，当然其他国家也可以搞一套自己的编码，结果就是互相之间谁也不懂谁的编码，我们能否将全世界所有的字符包含在一个集合里，计算机只要支持这一个字符集，就能显示所有的字符，让妈妈不再担心乱码。

可以，这种事儿当然只有ISO来做了。这就是ISO（国际标谁化组织）推出的 Unicode 。

> unicode开始制订时，计算机的存储器容量极大地发展了，空间再也不成为问题了。于是 ISO 就直接规定必须用两个字节，也就是16位来统一表示所有的字符

> Unicode其实是不断更新的，截止目前最新版 [Unicode 9.0.0](http://www.unicode.org/versions/Unicode9.0.0/#Technical_Overview)已经达到了 128,172 个字符。

Unicode从0开始，为每个符号指定一个编号，这叫做"码点"（code point）

> [@see 码表查询](http://www.unicode.org/charts/) 汉字在 East Asian Scripts —— CJK 部分（CJK Unified Ideographs / 中日韩统一表意文字）

### JS中的编码

JavaScript内部，字符以UTF-16的格式储存，每个字符固定为2个字节，JavaScript 允许采用字符串 '\uxxxx' 形式表示一个字符，其中xxxx为字符的 Unicode 码点(十六进制)。

比如我们在ES5中可以使用charCodeAt([index])函数得到字符的十进制码点，转换成16进制后可以对照Unicode官网的码表，或者直接使用'\uxxxx'格式表示字符，会输出字符本身

```js
'a'.charCodeAt();
// 97 // 得到10进制码点97
(97).toString(16)
// "61" // 转换成16进制输出61
'\u0061' // '\u{61}'
// "a" // 使用'\uxxxx'格式表示字符，输出字符串 a
"\u0061".charCodeAt().toString(16)
// "61"
```

从0000 - FFFF 共65535个可表示字符，charCodeAt() 方法可返回指定位置的字符的 Unicode 编码。这个返回值是十进制 0 - 65535 之间的整数。我们从Unicode官网可以看到最新的Unicode 9.0.0版本已经达到了128,172个字符

> Unicode的长度跟下方码点134071有出入，待核实是否是因为某些区（plate）未满，预留而导致码点比实际长度大的问题

```js
0xFFFF
// 65535
128172 - 0xFFFF
// 62637
```

必然有62637个不在范围中，超出的部分怎么办？比如下方的情况跟预期就不符合，一个汉字，输出其长度却提示 2 

```js
'𠮷'.length
// 2
```

究其原因其实就是用16个位即两个字节无法表述所有的字符，超出部分的码点比如 "𠮷" 的码点 20BB7 需要用4个字节也就是两个JS字符的长度来表示

ES6中码点大于0xFFFF无法识别的问题得到了比较好的解决

```js
"\uD842\uDFB7"
// "𠮷"
"\u20BB7"
// " 7"
"\uD842\uDFB7".charCodeAt(0)
// 55362
"\uD842\uDFB7".charCodeAt(0).toString(16)
// "d842"   // 丢失了dfB7
"\uD842\uDFB7".codePointAt(0)  // ES6函数
// 134071
(134071).toString(16)
// "20bb7"
```

> 上面代码表示，如果直接在\u后面跟上超过0xFFFF的数值（比如\u20BB7），JavaScript会理解成\u20BB+7。由于\u20BB是一个不可打印字符，所以只会显示一个空格，后面跟着一个7。

```js
var s = '𠮷a';
s.codePointAt(0) // 134071
s.codePointAt(1) // 57271
s.codePointAt(2) // 97
```

js中表示一个字符串的几种写法

```js
'\z' === 'z'  
// true
'\172' === 'z' 
// true
'\x7A' === 'z' // 16进制 固定一个字节(8位2进制，也就是两位16进制)，前255个unicode编码
// true 
'\u007A' === 'z' // unicode
// true Unicode
'\u{7A}' === 'z' // es6 unicode 写法
// true
```

什么是 UTF-8 UTF-16，为什么要有UTF-8 和 UTF-16？

<details>
<summary>查看</summary>

> unicode同样也不完美，这里就有两个问题，一个是，如何才能区别unicode和ascii？计算机怎么知道三个字节表示一个符号，而不是分别表示三个符号呢？第二个问题是，我们已经知道，英文字母只用一个字节表示就够了，如果unicode统一规定，每个符号用三个或四个字节表示，那么每个英文字母前都必然有二到三个字节是0，这对于存储空间来说是极大的浪费，文本文件的大小会因此大出二三倍，这是难以接受的。unicode在很长一段时间内无法推广，直到互联网的出现，为解决unicode如何在网络上传输的问题，于是面向传输的众多 UTF（UCS Transfer Format）标准出现了，顾名思义，UTF-8就是每次8个位传输数据，而UTF-16就是每次16个位。UTF-8就是在互联网上使用最广的一种unicode的实现方式，这是为传输而设计的编码，并使编码无国界，这样就可以显示全世界上所有文化的字符了。

> UTF-8 每次8个位传输数据也就是一个字节，在编译以及解析上 UTF-8 一共能用四个字节来表示。 但一般字符用三个字节就能满足了。

> 一个字节等于8位。从 00000000 - 11111111 这个就是一个字节的数值范围。换算成十进制就是 0-255。

> UTF-8之一字节：在UTF-8里面一个字节存的还是字母、数字和ASCII编码一样。所以编码范围也是0-127。为啥是127呢？一个字节换算成二进制不是 255吗？因为一字节的时候，第一位给借去了，第一位的值为固定为0。

> 两个字节能表示更多的字符，但也有借位，固定为高位的 110xxxxx、低位的10xxxxxx，比如其中的高位字节也就是第一个字节 110xxxxx 那么也就是范围从   11000000 - 11011111 换成十六进制范围是 C2 - DF。

> 三、四个字节原理同两个字节一样。

</details>

### 进一步的编码

Base64

Base64是一种用64个字符来表示任意二进制数据的方法。常用于在URL、Cookie、网页中传输少量二进制数据。

1 encodeURI() encodeURIComponent()

2 escape() unescape() // 已废弃 [@see MDN](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/escape)

```js
escape("测试文字")
// "%u6D4B%u8BD5%u6587%u5B57"
unescape("%u6D4B%u8BD5%u6587%u5B57")
// "测试文字"
"%u6D4B%u8BD5%u6587%u5B57".replace(/%u/g,'\\u')
// "\u6D4B\u8BD5\u6587\u5B57"
"\u6D4B\u8BD5\u6587\u5B57"
// "测试文字"
```

3 btoa() atob()

4 (num).toString(2-36)

5 hash MD5 SHA1 crc


### 计算机并不可靠

> 算法不能将所有的数字或函数计算到任意精度。事实上，人们能够证明，在算法上不可计算的函数要比可计算的函数要多。换句话说，出乎现代人意料之外 —— 计算机并不能解决所有的问题。

JS 数字处理的最大值

```js
Number.MAX_SAFE_INTEGER
// 9007199254740991
```

> 超出这个并不是说一定不能使用了，只是不安全了，在这个规定中能安全的表示数字的范围在 -(2的53次方 - 1) 到 2的53次方 - 1 之间，包含 -(2的53次方 - 1) 和 2的53次方 - 1,超出范围建议用字符串表示，比如我们公司就曾经出现了ID位数超出导致丢失精度的问题

```js
Number.MAX_SAFE_INTEGER + 1 == Number.MAX_SAFE_INTEGER + 2
// true // 结果为true，并不符合常理

var a = Math.cos(.5);
var b = Math.acos(a);
console.log(b)
// 0.4999999999999999 // 从数学上b应该等于0.5，但是从计算机上并不等于，因为丢失了精度
```



参考

* [Unicode 官网](http://www.unicode.org/)
* [阮一峰](http://www.ruanyifeng.com/blog/2014/12/unicode.html)
* [知乎](https://www.zhihu.com/question/23374078)